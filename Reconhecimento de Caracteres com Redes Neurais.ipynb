{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(data, width, height):\n",
    "    image = numpy.array(data).reshape(width, height)\n",
    "    plt.imshow(image, cmap=plt.cm.gray, interpolation=\"nearest\")\n",
    "    input('Press any key to continue!')\n",
    "\n",
    "def save(fname, data, width, height):\n",
    "    image = numpy.array(data).reshape(width, height)\n",
    "    plt.imsave(fname, image, cmap=plt.cm.gray)\n",
    "\n",
    "def view_weights(weights, width, height):\n",
    "    cdict = {\n",
    "        'red':\n",
    "        [\n",
    "            (0.0,  1.0, 1.0),\n",
    "            (0.25,  1.0, 1.0),\n",
    "            (0.5,  0.0, 0.0),\n",
    "            (1.0,  0.0, 0.0)\n",
    "        ],\n",
    "        'green':\n",
    "        [\n",
    "            (0.0,  0.0, 0.0),\n",
    "            (0.5,  0.0, 0.0),\n",
    "            (0.75, 1.0, 1.0),\n",
    "            (1.0,  1.0, 1.0)\n",
    "        ],\n",
    "        'blue':\n",
    "        [\n",
    "            (0.0,  0.0, 0.0),\n",
    "            (1.0,  0.0, 0.0)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    red_green = matplotlib.colors.LinearSegmentedColormap('green_red', cdict, 256)\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        img = weights.flatten()[i::10].reshape((width, height))\n",
    "        plt.imshow(img, cmap = red_green, clim=(-1, 1))\n",
    "        if i == 0:\n",
    "            pyplot.colorbar()\n",
    "        input('Class ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import dtypes, random_seed\n",
    "\n",
    "#DataSet constructor, used to store an monochromatic image based dataset\n",
    "#dtype can be either uint8 to leave the input as [0, 255], or float32 to rescale into [0, 1].\n",
    "#Seed arg provides for convenient deterministic testing.\n",
    "class DataSet(object):\n",
    "    def __init__(self, images, labels, dtype = dtypes.float32, reshape = True, seed = None):\n",
    "        seed1, seed2 = random_seed.get_seed(seed)\n",
    "\n",
    "        #If op level seed is not set, use whatever graph level seed is returned\n",
    "        numpy.random.seed(seed1 if seed is None else seed2)\n",
    "        dtype = dtypes.as_dtype(dtype).base_dtype\n",
    "\n",
    "        if dtype not in (dtypes.uint8, dtypes.float32):\n",
    "            raise TypeError('Invalid image dtype %r, expected uint8 or float32' % dtype)\n",
    "\n",
    "        assert images.shape[0] == labels.shape[0], ('images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\n",
    "\n",
    "        self.num_examples = images.shape[0]\n",
    "\n",
    "        #Convert shape from [num examples, rows, columns, depth] to [num examples, rows*columns](assuming depth == 1)\n",
    "        if reshape:\n",
    "            assert images.shape[3] == 1\n",
    "            images = images.reshape(images.shape[0], images.shape[1] * images.shape[2])\n",
    "\n",
    "        #Convert from [0, 255] -> [0.0, 1.0].\n",
    "        if dtype == dtypes.float32:\n",
    "            images = images.astype(numpy.float32)\n",
    "            images = numpy.multiply(images, 1.0 / 255.0)\n",
    "\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.epochs_completed = 0\n",
    "        self.index_in_epoch = 0\n",
    "\n",
    "    #Return the next batch_size examples from this data set\n",
    "    def next_batch(self, batch_size, shuffle=True):\n",
    "        start = self.index_in_epoch\n",
    "\n",
    "        #Shuffle for the first epoch\n",
    "        if self.epochs_completed == 0 and start == 0 and shuffle:\n",
    "            perm0 = numpy.arange(self.num_examples)\n",
    "            numpy.random.shuffle(perm0)\n",
    "            self.images = self.images[perm0]\n",
    "            self.labels = self.labels[perm0]\n",
    "\n",
    "        #Get Next epoch\n",
    "        if start + batch_size > self.num_examples:\n",
    "            #Finished epoch\n",
    "            self.epochs_completed += 1\n",
    "\n",
    "            #Get the rest examples in this epoch\n",
    "            rest_num_examples = self.num_examples - start\n",
    "            images_rest_part = self.images[start:self.num_examples]\n",
    "            labels_rest_part = self.labels[start:self.num_examples]\n",
    "\n",
    "            #Shuffle the data\n",
    "            if shuffle:\n",
    "                perm = numpy.arange(self.num_examples)\n",
    "                numpy.random.shuffle(perm)\n",
    "                self.images = self.images[perm]\n",
    "                self.labels = self.labels[perm]\n",
    "\n",
    "            #Start next epoch\n",
    "            start = 0\n",
    "            self.index_in_epoch = batch_size - rest_num_examples\n",
    "            end = self.index_in_epoch\n",
    "            images_new_part = self.images[start:end]\n",
    "            labels_new_part = self.labels[start:end]\n",
    "            return numpy.concatenate((images_rest_part, images_new_part), axis=0) , numpy.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "        else:\n",
    "            self.index_in_epoch += batch_size\n",
    "            end = self.index_in_epoch\n",
    "            return self.images[start:end], self.labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy\n",
    "from random import shuffle\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "Datasets = collections.namedtuple('Datasets', ['train', 'validation', 'test'])\n",
    "\n",
    "def read_data_semeion(fname = 'data/semeion.data'):\n",
    "    file = open(fname, 'r')\n",
    "    lines = file.readlines()\n",
    "\n",
    "    width = 16\n",
    "    height = 16\n",
    "    size = width * height\n",
    "    classes = 10\n",
    "\n",
    "    images = [];\n",
    "    labels = [];\n",
    "    fnumber = 0;\n",
    "\n",
    "    for line in lines:\n",
    "        data = line.split(' ')\n",
    "        image = [];\n",
    "        label = [];\n",
    "\n",
    "        for i in range(0, size):\n",
    "            image.append(int(float(data[i])))\n",
    "        images.append(image)\n",
    "\n",
    "        for i in range(size, size + classes):\n",
    "            label.append(int(float(data[i]))) \n",
    "        labels.append(label)\n",
    "\n",
    "        fnumber += 1\n",
    "        #if fnumber < 10:\n",
    "            #image_utils.show(image, width, height)\n",
    "        #image_utils.save('./dataset/semeion/images/' + str(fnumber) + '.png', array, width, height)\n",
    "\n",
    "    #Shuffle data\n",
    "    images_shuffle = []\n",
    "    labels_shuffle = []\n",
    "    indexes = list(range(len(images)))\n",
    "    shuffle(indexes)\n",
    "    for i in indexes:\n",
    "        images_shuffle.append(images[i])\n",
    "        labels_shuffle.append(labels[i])\n",
    "\n",
    "    images = images_shuffle\n",
    "    labels = labels_shuffle\n",
    "\n",
    "    samples = len(lines)\n",
    "    train_samples = 1300\n",
    "    test_samples = 1100\n",
    "\n",
    "    #Train set\n",
    "    image_train = numpy.array(images[:train_samples], dtype=numpy.uint8)\n",
    "    image_train = image_train.reshape(train_samples, width, height, 1)\n",
    "\n",
    "    label_train = numpy.array(labels[:train_samples], dtype=numpy.uint8)\n",
    "\n",
    "    train = DataSet(image_train, label_train, reshape=True)\n",
    "\n",
    "    #test set\n",
    "    image_test = numpy.array(images[test_samples:], dtype=numpy.uint8)\n",
    "    image_test = image_test.reshape(samples - test_samples, width, height, 1)\n",
    "\n",
    "    label_test = numpy.array(labels[test_samples:], dtype=numpy.uint8)\n",
    "\n",
    "    test = DataSet(image_test, label_test, reshape=True)\n",
    "\n",
    "    return Datasets(train=train, validation=data, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 128 and 10 for 'Sub_6' (op: 'Sub') with input shapes: [?,128], [?,10].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1818\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1819\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1820\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 128 and 10 for 'Sub_6' (op: 'Sub') with input shapes: [?,128], [?,10].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-8600a62a1f3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmaprime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36msubtract\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m  11273\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11274\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m> 11275\u001b[1;33m         \"Sub\", x=x, y=y, name=name)\n\u001b[0m\u001b[0;32m  11276\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11277\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    798\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    799\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    801\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 instructions)\n\u001b[1;32m--> 507\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3477\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3479\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3480\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3481\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1981\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1982\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1983\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1985\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1820\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1822\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1824\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 128 and 10 for 'Sub_6' (op: 'Sub') with input shapes: [?,128], [?,10]."
     ]
    }
   ],
   "source": [
    "#Dataset\n",
    "width = 16\n",
    "height = 16\n",
    "dataset = read_data_semeion()\n",
    "\n",
    "#Parameters\n",
    "learning_rate = 0.001\n",
    "iterations = 500\n",
    "batch_size = 100\n",
    "\n",
    "#Layers number of features\n",
    "n_hidden_1 = 128\n",
    "n_hidden_2 = 128\n",
    "\n",
    "#Number of input and outputs\n",
    "n_input = width * height\n",
    "classes = 10\n",
    "\n",
    "#Data placeholders\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, classes])\n",
    "\n",
    "#Layers weights\n",
    "w1 = tf.Variable(tf.random_normal([n_input, n_hidden_1]))\n",
    "w2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
    "w_out = tf.Variable(tf.random_normal([n_hidden_2, classes]))\n",
    "\n",
    "#Layers bias\n",
    "b1 = tf.Variable(tf.random_normal([n_hidden_1]))\n",
    "b2 = tf.Variable(tf.random_normal([n_hidden_2]))\n",
    "b_out = tf.Variable(tf.random_normal([classes]))\n",
    "\n",
    "#Hidden layer 1\n",
    "z_1 = tf.add(tf.matmul(x, w1), b1)\n",
    "a_1 = tf.nn.sigmoid(z_1)\n",
    "\n",
    "#Hidden layer 2\n",
    "z_2 = tf.add(tf.matmul(a_1, w2), b2)\n",
    "a_2 = tf.nn.sigmoid(z_2)\n",
    "\n",
    "#Output layer with linear activation\n",
    "pred = tf.matmul(a_2, w_out) + b_out\n",
    "\n",
    "\n",
    "diff = tf.subtract(a_2, y)\n",
    "\n",
    "def sigmaprime(x):\n",
    "    return tf.multiply(tf.nn.sigmoid(x), tf.subtract(tf.constant(1.0), tf.nn.sigmoid(x)))\n",
    "\n",
    "\n",
    "d_z_2 = tf.multiply(diff, sigmaprime(a_2))\n",
    "d_b_2 = d_z_2\n",
    "d_w_2 = tf.matmul(tf.transpose(a_1), d_z_2)\n",
    "\n",
    "d_a_1 = tf.matmul(d_z_2, tf.transpose(w2))\n",
    "d_z_1 = tf.multiply(d_a_1, sigmaprime(z_1))\n",
    "d_b_1 = d_z_1\n",
    "d_w_1 = tf.matmul(tf.transpose(x), d_z_1)\n",
    "\n",
    "acct_mat = tf.equal(tf.argmax(a_2, 1), tf.argmax(y, 1))\n",
    "acct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n",
    "\n",
    "#Cost and optimizer\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#Start time\n",
    "start = time.time()\n",
    "\n",
    "#Session\n",
    "init = tf.global_variables_initializer()\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "\n",
    "#Training cycle\n",
    "for i in range(iterations):\n",
    "    total_batch = int(dataset.train.num_examples / batch_size)\n",
    "\n",
    "    #Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x, batch_y = dataset.train.next_batch(batch_size)\n",
    "\n",
    "        session.run(acct_res, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "    #Test\n",
    "    if i % 5 == 0:\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(\"Iteration: \" + str(i) + \" Accuracy: \", accuracy.eval(session=session, feed_dict={x: dataset.test.images, y: dataset.test.labels}))\n",
    "\n",
    "#End time\n",
    "end = time.time()\n",
    "\n",
    "#Test\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print(\"Accuracy:\", accuracy.eval(session=session, feed_dict={x: dataset.test.images, y: dataset.test.labels}))\n",
    "\n",
    "#Time\n",
    "print('Time: ' + str(end - start));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
